---
title: Simple linear regression
subtitle: "{{< var workshop-title >}}"
description: "[Slide 3]{.tag-workshop}"
format:
  anu-light-revealjs:
    width: 1920
    height: 1080
    auto-stretch: false
    html-math-method: katex
    self-contained: false
    title-slide-attributes:
        data-background-image: /images/slide3-cover.jpeg
        data-background-size: cover
    include-in-header: 
      - text: '<script src="/assets/leader-line.min.js"></script>'
    css: 
     - /assets/slides.css
    footer: "{{< var workshop-url >}}"
webr:
  cell-options:
    fig-width: 12
    fig-height: 8
  packages:
    - tidyverse
    - patchwork
    - faraway
    - skimr
    - broom
    - ggResidpanel
    - MASS
image: /images/slide3-cover.jpeg
author: Emi Tanaka
date: 2025/02/18
date-format: "D[th] MMMM YYYY"
execute: 
  echo: false
  fig-width: 12
  fig-height: 7
  fig-align: center
revealjs-plugins:
  - leader-line
---


```{r}
#| include: false
library(tidyverse)
library(patchwork)
source("setup.R")
theme_set(theme_bw(base_size = 24) + 
            theme(plot.title.position = "plot",
                  plot.background = element_rect(fill = "transparent", color = "transparent"),
                  legend.background = element_rect(fill = "transparent"),
                  panel.background = element_rect(fill = "transparent")))
options(ggplot2.discrete.fill = list(c("forestgreen", "red2")),
        ggplot2.discrete.colour = list(c("forestgreen", "red2")))
```


## Case study {{< fa solid database >}} Seed weight from seed length

::: {.columns}
::: {.column width="40%"}


- **Seed length** is expected to be a major contributor to differences in **seed weight** for wheat. 
- 190 seeds selected at random from a line of diploid wheat, _Triticum monococcum_ for length and weight.


::: {.f2 style="opacity:0.5"}

```{webr}
#| autorun: true
library(tidyverse)
theme_set(theme_bw(base_size = 24))
```

```{r}
#| autorun: true
#| echo: false
options(width = 75)
```

:::

:::

::: {.column width="60%"}

::: f2

```{webr}
#| warning: false
#| message: false
#| autorun: true
#| fig-height: 6
triticum <- read_table("https://emitanaka.org/workshop-intro-stats/data/TRITICUM.DAT")
ggplot(triticum) +
  aes(Length, Weight) +
  geom_point()
```

:::

:::
:::



::: aside 

Data source: Welham et al. (2015) Statistical Methods in Biology: Design and Analysis of Experiments and Regression.

:::


## Which line looks best for describing the relationship

```{r}
#| fig-width: 18
triticum <- read_table("https://emitanaka.org/workshop-intro-stats/data/TRITICUM.DAT")
betas <- coef(lm(Weight ~ Length, data = triticum))
bind_rows(mutate(id = "Line A", triticum),
          mutate(id = "Line B", triticum),
          mutate(id = "Line C", triticum)) |> 
  ggplot() +
  aes(Length, Weight) +
  geom_point() +
  geom_abline(aes(slope = slope, intercept = intercept, colour = id), size = 1.5,
              data = tibble(slope = c(round(betas[2]) + 0.1, betas[2], betas[2]),
                            intercept = c(betas[1], betas[1], round(betas[1]) - 0.5),
                            id = paste("Line", LETTERS[1:3]))) +
  facet_wrap(~id, scales = "free") +
  guides(colour = "none") + 
  colorspace::scale_color_discrete_qualitative()
```


## Simple linear regression


- We seek to model the relationship between:

  - the mean of a [**response variable**]{.blue}, $y$, and 
  - a single [**explanatory variable**]{.blue} (or [**predictor**]{.blue}/[**covariate**]{.blue}) $x$.

- For observations $i = 1, 2, \ldots, n$:

<center>

$y_i  =$ [$\beta_0$]{#beta0} $+$ [$\beta_1$]{#beta1}$x_i +$ [$\epsilon_i$]{#errori}

<br><br><br><br>


</center>

- $\boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\ \beta_1\end{bmatrix}$ are referred to as the [**regression parameters/coefficients**]{.blue}.

[intercept]{#intercept .absolute bottom="35%" left="20%"}
[slope]{#slope .absolute bottom="25%" left="50%"}
[error for the $i$-th observation, and assume $\epsilon_i \sim NID(0, \sigma^2)$, i.e. normally and independently distributed with mean 0 and variance $\sigma^2$.]{#error .absolute  .absolute bottom="35%" left="70%"}


[]{.leaderline start='#intercept' end='#beta0'  draw-effect="draw" end-socket="bottom" }
[]{.leaderline start='#slope' end='#beta1'  draw-effect="draw"}
[]{.leaderline start='#error' end='#errori'  draw-effect="draw"}


## Least Squares Estimates

::: {.columns}
::: {.column width="45%"}


```{webr}
#| fig-width: 8
#| fig-height: 7
#| echo: false
#| input:
#| - intercept_1
#| - slope_1
data <- triticum |> 
  mutate(.fitted = intercept_1 + slope_1 * Length,
         .resid = Weight - .fitted)
RSS <- sum(data$.resid^2)

data |> 
  ggplot(aes(Length, Weight)) +
  geom_abline(slope = slope_1, intercept = intercept_1, color = "lightgrey")  + 
  geom_segment(aes(xend = Length, yend = .fitted), alpha = .2) +
  geom_point(aes(color = .resid), size = 4) + 
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +  
  guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 2) +
  theme_bw(base_size=18) + 
  labs(x="Length", y="Weight", title = paste0("RSS = ", round(RSS, 2)))
```


:::

::: {.column width="55%"}

- Find $\hat \beta_0$ and $\hat \beta_1$ that minimize the
 sum of squares:

$$\text{RSS}(\beta_0, \beta_1) = \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2$$

- The least squares estimates can be found by using calculus.
- Visually, we can try changing the regression parameters below:

```{ojs}
viewof intercept_1 = Inputs.number({step: 0.1, value: -27.9, label: "β̂₀"})
viewof slope_1 = Inputs.number({step: 0.1, value: 17.2, label: "β̂1"})
```






:::
:::





## Fitting linear models in R

$$\texttt{Weight}_i=\beta_0 + \beta_1\texttt{Length}_i + e_i$$

```{webr}
#| autorun: true
fit <- lm(Weight ~ Length, data = triticum)
fit
```

- The least square estimates are: $\hat{\beta}_0 = `r scales::comma(betas[1], 0.01)`$ and $\hat{\beta}_1 = `r scales::comma(betas[2], 0.01)`$.






## Fitted values

::: {.columns}
::: {.column width="50%"}


```{r}
#| fig-height: 8
#| echo: false
fit <- lm(Weight ~ Length, data = triticum)
triticum |> 
  mutate(fitted = fitted(fit)) |> 
  ggplot(aes(Length, Weight)) +
  geom_point(size = 4)  +
  geom_segment(aes(xend = Length, yend = fitted), alpha = .2) +
  geom_line(aes(y = fitted), color = "red") +
  geom_point(aes(y = fitted), size = 4, color = "red")
```



:::

::: {.column width="50%"}

- The fitted values are the red points given as:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i.$$

:::
:::

```{webr}
fitted(fit)
```


## Predicted values 

::: {.columns}
::: {.column width="40%"}


```{r}
#| fig-height: 8
#| echo: false
preddata <- data.frame(Length = seq(2.5, 4, 0.5)) |> 
  mutate(predict = predict(fit, newdata = data.frame(Length = Length)))
triticum |> 
  ggplot(aes(Length, Weight)) +
  geom_point(size = 4)  +
  geom_smooth(method = "lm", color = "red", se = FALSE) + 
  geom_point(aes(y = predict),
             size = 6, color = "red",
             data = preddata)
```



:::

::: {.column width="60%"}

- The response can be predicted from the fitted model for a given $x$ as:

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.$$



:::
:::

```{webr}
# output = vector
predict(fit, newdata = data.frame(Length = c(2.5, 3, 3.5, 4))) 
```

```{webr}
# output = tibble
broom::augment(fit, newdata = data.frame(Length = c(2.5, 3, 3.5, 4))) 
```




## Making inferences

::: incremental

- The standard deviation of an estimator is referred to as the [**standard error**]{.blue}.
- In making inferences, you may like to know:
  - what is the standard error of $\hat{\beta}_1$?
  - is $\beta_1 \neq 0$?
  - what is the confidence interval of the average $y$ for a given $x$?
  - what is the prediction interval of $y$ for a given $x$?
- <i class="fas fa-pen-square"></i> It's important to note that making **inferences require the assumptions of the linear regression model to be satisfied**.
- So check model assumptions first!

:::

## Assumptions for linear regression

- Recall for $i=1,\ldots,n$, $\epsilon_i \sim NID(0,\sigma^2)$ which means:
  - (A1) $\text{E}(\epsilon_i) = 0$ 
  - (A2) $\epsilon_1, \ldots ,\epsilon_n$ are [independent]{.blue}.
  - (A3) $\text{Var}(\epsilon_i) = \sigma^2$ which is called the [homoscedasticity assumption]{.blue}.
  - (A4) $\epsilon_1, \ldots ,\epsilon_n$ are [normally distributed]{.blue}
  - (A5) the [predictors are known without error]{.blue}. 

. . .   

- An unbiased estimate of $\sigma$ is given by:

```{webr}
sigma(fit)
```

## Checking assumptions

- A1 is satisfied by definition for the least squares estimates.

```{webr}
all.equal(sum(residuals(fit)), 0) # virtually equal to 0
```

. . . 

- A2 and A3 can be checked by plotting the residuals against the fitted values and ensuring that there is no pattern or trends.

```{webr}
ggplot(broom::augment(fit), aes(.fitted, .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, color = "red")
```

. . . 

- Also checked by plotting the residuals against the predictor(s).

```{webr}
ggplot(broom::augment(fit), aes(Length, .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, color = "red")
```

. . . 

- Sometimes plotting the residuals against the order of data entry can also be useful in identifying potential violations of A2.

```{webr}
broom::augment(fit) |> 
  mutate(index = 1:n()) |> 
  ggplot(aes(index, .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, color = "red")
```

. . . 

- A4 can be checked by plotting the normal quantile-quantile plot of the residuals. If it roughly straight then A4 is satisfied. 

```{webr}
ggplot(broom::augment(fit), aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line()
```

- There are no standard ways to check A5 easily. 



## Influence measures

- The data may contain outliers, high leverage values, and other unusual values that can affect the regression model.

```{webr}
#| autorun: true
infm <- influence.measures(fit)
i_check <- which(apply(infm$is.inf, 1, any))
```

. . . 

- Let's check the "influential" observations:

```{webr}
ggplot(triticum, aes(Length, Weight)) +
  geom_point(size = 2) + 
  geom_point(data = ~. |> slice(i_check), color = "red", size = 2) + 
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) 
```

. . . 

- [Red points]{style="color:red"} are the observations to scrutinize further. 

## Box cox transformation 

::: {.columns}
::: {.column width="50%"}

- Box-cox transformation modifies the response for a given value of $\lambda$ such that:

$$y(\lambda) = \begin{cases} \frac{y^{\lambda} - 1}{\lambda} & \text{if } \lambda \neq 0, \\ \log(y) & \text{if } \lambda = 0. \end{cases}$$

:::

::: {.column width="50%"}

- The transformation is equivalent to:

| $\lambda$ | Transformation |
|:---------:|:--------------:|
| $2$ | $y^2$ |
| $1$ | $y$ |
| $0.5$ | $\sqrt{y}$ |
| $0$ | $\log(y)$ |
| $-0.5$ | $\sqrt{y}$ |
| $-1$ | $\frac{1}{y}$ |
| $-2$ | $\frac{1}{y^2}$ |

:::
:::


## Selecting $\lambda$ for box-cox transformation


```{webr}
#| fig-height: 6
MASS::boxcox(fit)
```

. . . 

- Profile log-likelihood plot suggests $\lambda \approx 0.5$ which is equivalent to taking the square root of the response.

## Transforming the response

```{webr}
#| autorun: true
fit_sqrt <- lm(sqrt(Weight) ~ Length, data = triticum)
```

. . . 

- Remember the fitted or predicted value need to be squared to get the original scale.


```{webr}
fitted(fit_sqrt)^2
```



## Quick diagnostic plots

- An easy way to generate some of the diagnostic plots at once is to use the `ggResidpanel` package.

```{webr}
ggResidpanel::resid_panel(fit)
```

. . . 

- But the QQ-plot still doesn't look good enough??

## Visual inference

- When making inference from plots, it's best to **calibrate the plot** with simulations.
- Let's assume that $\sqrt{\texttt{Weight}} = \hat{\beta}_0 + \hat{\beta}_1 \texttt{Length} + \epsilon$, where $\epsilon \sim N(0, \hat{\sigma}^2)$ is the correct model. 
- Then simulate from this model 9 times:

```{webr}
#| autorun: true
sims <- map_dfr(1:14, \(i) {
  simdata <- triticum |> 
    mutate(y = fitted(fit_sqrt) + rnorm(n(), 0, sd = sigma(fit_sqrt)))
  fit_sim <- lm(y ~ Length, data = simdata)
  broom::augment(fit_sim) |> 
    mutate(sim = i)
})
# add the actual one
sims <- bind_rows(sims, broom::augment(fit_sqrt))
```

- Then let's look at the QQ-plot of the residuals from the simulated data:

```{webr}
#| fig-width: 16
#| fig-height: 11
ggplot(sims, aes(sample = .resid)) +
  stat_qq() + 
  stat_qq_line() +
  facet_wrap(~sim, nrow = 3)
```



## <i class="fab fa-r-project"></i> Functions for model object

- Getting the model summary:

```{webr}
summary(fit_sqrt)
```

- Getting just the regression coefficient estimates 

```{webr}
coef(fit_sqrt) # or fit_sqrt$coef
```
- Getting the regression coefficient table in the model summary as a tibble:


```{webr}
broom::tidy(fit_sqrt)
```


- Getting the fit_sqrtted values $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$:

```{webr}
fit_sqrtted(fit_sqrt)
```

- Getting the residuals $(y_i - \hat{y}_i)$:

```{webr}
residuals(fit_sqrt)
```

- Augment the data with fit_sqrtted values, residuals, etc:

```{webr}
broom::augment(fit_sqrt)
```


- Getting the deviance (residual sum of squares):

```{webr}
deviance(fit_sqrt) # same as sum(residuals(fit_sqrt)^2)
```

- The estimate of the error standard deviation $\hat{\sigma}$:

```{webr}
sigma(fit_sqrt)
```

- Getting a single numerical summaries about the model (e.g. coefficient of determination, adjusted $R^2$, AIC, BIC, etc):

```{webr}
broom::glance(fit_sqrt)
```

- Getting the influence measures and find which observations are influential:

```{webr}
infm <- influence.measures(fit_sqrt)
which(apply(infm$is.inf, 1, any))
```

- Selecting $\lambda$ for box-cox transformation:

```{webr}
MASS::boxcox(fit_sqrt)
```

- Quick diagnostic plots:

```{webr}
ggResidpanel::resid_panel(fit_sqrt)
```



